{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect if the outputs from the 10 random runs deviate significantly from each other. \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from netCDF4 import Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score as R2\n",
    "from sklearn.model_selection import KFold\n",
    "from copy import deepcopy\n",
    "import utils\n",
    "from unet import UNet_nobatchnorm\n",
    "from scipy.stats import pearsonr\n",
    "from pathlib import Path\n",
    "import numpy.fft as fft\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import helper_functions as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/work/uo0780/u241359/project_tide_synergy/data/'\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "Ntrain = np.sum([nc.dimensions['time_counter'].size for nc in nctrains], axis = 0)\n",
    "Ntest = np.sum([nc.dimensions['time_counter'].size for nc in nctest], axis = 0)\n",
    "\n",
    "model_folder = '/work/uo0780/u241359/project_tide_synergy/trainedmodels_forpaper/'\n",
    "#filesuffix1 + randomseednumber + filesuffix2 = file suffix. \n",
    "filesuffix1='_ssh_cosssh_sin_nobatchnorm_rp_'\n",
    "filesuffix2='.pth'\n",
    "\n",
    "vel_cmap  = 'BrBG' #'viridis'\n",
    "vort_cmap = 'PRGn'\n",
    "ssh_cmap  = 'bwr'\n",
    "sst_cmap = 'inferno'\n",
    "\n",
    "bottom_slice = slice(0,256)\n",
    "mid_slice = slice(232, 488)\n",
    "top_slice = slice(464, 720)\n",
    "\n",
    "def corr(data, mod):\n",
    "    return pearsonr(data.flatten(), mod.flatten())[0]\n",
    "def L2_R(data,mod):\n",
    "    return R2(data.flatten(), mod.flatten())\n",
    "\n",
    "nensemble = 10 #How many re-trained U-Net models there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbase = 16\n",
    "def totorch(x):\n",
    "    return torch.tensor(x, dtype = torch.float).cpu()\n",
    "    \n",
    "def preload_data(nctrains, total_records):\n",
    "    #total_records = Ntrain#sum(nc.dimensions['time_counter'].size for nc in nctrains)\n",
    "    #dimensions of data of the nc file.\n",
    "    max_height = 722\n",
    "    max_width = 258\n",
    "    all_input_data = np.zeros((total_records, N_inp, max_height, max_width))*np.nan\n",
    "    all_output_data = np.zeros((total_records, N_out, max_height, max_width))*np.nan\n",
    "    current_index = 0\n",
    "    for ncindex, ncdata in enumerate(nctrains):\n",
    "        num_recs = ncdata.dimensions['time_counter'].size\n",
    "        rec_slice = slice(current_index, current_index + num_recs)\n",
    "        \n",
    "        for ind, var_name in enumerate(var_input_names):\n",
    "            data_slice = np.squeeze(ncdata.variables[var_name])\n",
    "            # print('data_slice shape:')\n",
    "            # print(data_slice.shape)        \n",
    "            #all_input_data[rec_slice, ind, :, :] = data_slice\n",
    "            #For some variables, the dimensions in (x, y) may be smaller than (max_height, max_width). Changing the code so that it adapts them.\n",
    "            # Get the actual dimensions of data_slice\n",
    "            slice_height, slice_width = data_slice.shape[-2], data_slice.shape[-1]\n",
    "            # Place data_slice into the corresponding slice of all_input_data\n",
    "            all_input_data[rec_slice, ind, :slice_height, :slice_width] = data_slice\n",
    "    \n",
    "\n",
    "        for ind, var_name in enumerate(var_output_names):\n",
    "            data_slice = np.squeeze(ncdata.variables[var_name])\n",
    "            #all_output_data[rec_slice, ind, :, :] = data_slice\n",
    "            # Get the actual dimensions of data_slice\n",
    "            slice_height, slice_width = data_slice.shape[-2], data_slice.shape[-1]\n",
    "            # Place data_slice into the corresponding slice of all_input_data\n",
    "            all_output_data[rec_slice, ind, :slice_height, :slice_width] = data_slice\n",
    "\n",
    "        current_index += num_recs\n",
    "        \n",
    "    return all_input_data, all_output_data\n",
    "\n",
    "# # Modify the loadtrain function to pull data from preloaded memory\n",
    "# def loaddata_preloaded_train(index, batch_size, all_input_data, all_output_data):\n",
    "#     rec_slice = slice(index, index + batch_size)\n",
    "#     lim = 720\n",
    "#     width = 256\n",
    "#     yslice = slice(0, lim)\n",
    "#     xslice = slice(0, width)\n",
    "#     # print('rec_slice is:')\n",
    "#     # print(rec_slice)\n",
    "#     # print('mean of squared values of loaded input data:')\n",
    "#     # print(\"{0:0.32f}\".format(np.nanmean(all_input_data[rec_slice, :, yslice, xslice]**2)))\n",
    "#     return (all_input_data[rec_slice, :, yslice, xslice], \n",
    "#             all_output_data[rec_slice, :, yslice, xslice])\n",
    "#Load test data as one single batch\n",
    "def loaddata_preloaded_test(all_input_data, all_output_data):\n",
    "    #rec_slice = slice(index, index + batch_size)\n",
    "    lim = 720\n",
    "    width = 256\n",
    "    yslice = slice(0, lim)\n",
    "    xslice = slice(0, width)\n",
    "    # print('rec_slice is:')\n",
    "    # print(rec_slice)\n",
    "    # print('mean of squared values of loaded input data:')\n",
    "    # print(\"{0:0.32f}\".format(np.nanmean(all_input_data[rec_slice, :, yslice, xslice]**2)))\n",
    "    return (all_input_data[:, :, yslice, xslice], \n",
    "            all_output_data[:, :, yslice, xslice])\n",
    "\n",
    "\n",
    "def load_variable(ncdata, ncindex, variable, rec_slice, yslice, xslice):\n",
    "    data_squeezed = np.squeeze(ncdata[ncindex].variables[variable])\n",
    "    return data_squeezed[rec_slice, yslice, xslice]\n",
    "\n",
    "def hwvorticity(u, v, dgrid = 4000):\n",
    "    return (np.gradient(v, axis =2) - np.gradient(u, axis =1))/dgrid\n",
    "\n",
    "def hwdivergence(u, v, dgrid = 4000):\n",
    "    return (np.gradient(u, axis =2) + np.gradient(v, axis =1))/dgrid\n",
    "\n",
    "def preload_data_vortdiv(nctrains, total_records):\n",
    "    #total_records = Ntrain#sum(nc.dimensions['time_counter'].size for nc in nctrains)\n",
    "    #dimensions of data of the nc file.\n",
    "    max_height = 722\n",
    "    max_width = 258\n",
    "    all_input_data = np.zeros((total_records, N_inp, max_height, max_width))*np.nan\n",
    "    all_output_data = np.zeros((total_records, N_out, max_height, max_width))*np.nan\n",
    "    current_index = 0\n",
    "    for ncindex, ncdata in enumerate(nctrains):\n",
    "        num_recs = ncdata.dimensions['time_counter'].size #how many time stamps are there in each .nc file (i.e., at each turbulence level)\n",
    "        rec_slice = slice(current_index, current_index + num_recs)\n",
    "        for ind, var_name in enumerate(var_input_names):\n",
    "            if var_name == 'vort':\n",
    "                u = np.squeeze(ncdata.variables['u_xy_ins'])\n",
    "                v = np.squeeze(ncdata.variables['v_xy_ins'])\n",
    "                #u.shape: (150, 722, 257); v.shape: (150, 721, 258)\n",
    "                #as u and v have different number of grid points in x and y, we truncate them so that their shapes agree, enabling the simple way to compute vorticities based on finite diff.\n",
    "                data_slice = hwvorticity(u[:,:-1,:], v[:,:,:-1])\n",
    "            elif var_name == 'div':\n",
    "                u = np.squeeze(ncdata.variables['u_xy_ins'])\n",
    "                v = np.squeeze(ncdata.variables['v_xy_ins'])\n",
    "                data_slice = hwdivergence(u[:,:-1,:], v[:,:,:-1])\n",
    "            else:           \n",
    "                data_slice = np.squeeze(ncdata.variables[var_name])\n",
    "            # print('data_slice shape:')\n",
    "            # print(data_slice.shape)        \n",
    "            #all_input_data[rec_slice, ind, :, :] = data_slice\n",
    "            \n",
    "            #For some variables, the dimensions in (x, y) may be smaller than (max_height, max_width). Changing the code so that it adapts them.\n",
    "            # Get the actual dimensions of data_slice\n",
    "            slice_height, slice_width = data_slice.shape[-2], data_slice.shape[-1]\n",
    "            # Place data_slice into the corresponding slice of all_input_data\n",
    "            all_input_data[rec_slice, ind, :slice_height, :slice_width] = data_slice\n",
    "    \n",
    "\n",
    "        for ind, var_name in enumerate(var_output_names):\n",
    "            data_slice = np.squeeze(ncdata.variables[var_name])\n",
    "            #all_output_data[rec_slice, ind, :, :] = data_slice\n",
    "            # Get the actual dimensions of data_slice\n",
    "            slice_height, slice_width = data_slice.shape[-2], data_slice.shape[-1]\n",
    "            # Place data_slice into the corresponding slice of all_input_data\n",
    "            all_output_data[rec_slice, ind, :slice_height, :slice_width] = data_slice\n",
    "\n",
    "        current_index += num_recs\n",
    "        \n",
    "    return all_input_data, all_output_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full panel, correlation:\n",
      ", full & 0.7908 & 0.7964 & 0.7868 & 0.0029 \\\\\n",
      "full panel, R2:\n",
      ", full & 0.6235 & 0.6338 & 0.6175 & 0.0046 \\\\\n",
      "midjet panel, correlation:\n",
      ", mid & 0.6781 & 0.6872 & 0.6689 & 0.0059 \\\\\n",
      "midjet panel, R2:\n",
      ", mid & 0.4544 & 0.4689 & 0.4431 & 0.0078 \\\\\n"
     ]
    }
   ],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Change below for each Configuration ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "vi1 = 'ssh_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}_{}{}_nobatchnorm'.format(vi1, vo1, vo2)\n",
    "var_input_names = [vi1]\n",
    "var_output_names = [vo1, vo2]\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "\n",
    "inp_test, out_test = loaddata_preloaded_test(all_test_input, all_test_output)\n",
    "out_test = out_test*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "truth_bot = out_test[:, :, bottom_slice, :]\n",
    "truth_mid = out_test[:, :, mid_slice, :]\n",
    "truth_top = out_test[:, :, top_slice, :]\n",
    "\n",
    "combined_names = ''.join(var_input_names)\n",
    "\n",
    "#Array to record performance metrics\n",
    "corr_ensemble_full = np.zeros(nensemble)\n",
    "R2_ensemble_full = np.zeros(nensemble)\n",
    "corr_ensemble_top = np.zeros(nensemble)\n",
    "R2_ensemble_top = np.zeros(nensemble)\n",
    "corr_ensemble_mid = np.zeros(nensemble)\n",
    "R2_ensemble_mid = np.zeros(nensemble)\n",
    "corr_ensemble_bot = np.zeros(nensemble)\n",
    "R2_ensemble_bot = np.zeros(nensemble)\n",
    "for iensemble in np.arange(nensemble):\n",
    "    model_filename = f'any_{combined_names}{filesuffix1}{iensemble}{filesuffix2}'\n",
    "    model_path = Path(model_folder, model_filename)\n",
    "    state_dict = torch.load(model_path)\n",
    "    # Create a new instance of the model\n",
    "    model = UNet_nobatchnorm(N_inp, N_out, bilinear = True, Nbase = Nbase)\n",
    "    # Load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_mod = model(totorch(inp_test)).detach().cpu().numpy()\n",
    "    #Renormalize\n",
    "    out_mod = out_mod*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "    mod_bot = out_mod[:, :, bottom_slice, :]\n",
    "    mod_mid = out_mod[:, :, mid_slice, :]\n",
    "    mod_top = out_mod[:, :, top_slice, :]\n",
    "\n",
    "    corr_ensemble_full[iensemble] = corr(out_test, out_mod)\n",
    "    R2_ensemble_full[iensemble] = L2_R(out_test, out_mod)    \n",
    "    corr_ensemble_top[iensemble] = corr(truth_top, mod_top)\n",
    "    R2_ensemble_top[iensemble] = L2_R(truth_top, mod_top)  \n",
    "    corr_ensemble_mid[iensemble] = corr(truth_mid, mod_mid)\n",
    "    R2_ensemble_mid[iensemble] = L2_R(truth_mid, mod_mid)\n",
    "    corr_ensemble_bot[iensemble] = corr(truth_bot, mod_bot)\n",
    "    R2_ensemble_bot[iensemble] = L2_R(truth_bot, mod_bot)\n",
    "\n",
    "print('full panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_full)\n",
    "max_val = np.max(corr_ensemble_full)\n",
    "min_val = np.min(corr_ensemble_full)\n",
    "std_val = np.std(corr_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('full panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_full)\n",
    "max_val = np.max(R2_ensemble_full)\n",
    "min_val = np.min(R2_ensemble_full)\n",
    "std_val = np.std(R2_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_mid)\n",
    "max_val = np.max(corr_ensemble_mid)\n",
    "min_val = np.min(corr_ensemble_mid)\n",
    "std_val = np.std(corr_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_mid)\n",
    "max_val = np.max(R2_ensemble_mid)\n",
    "min_val = np.min(R2_ensemble_mid)\n",
    "std_val = np.std(R2_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full panel, correlation:\n",
      ", full & 0.9708 & 0.9763 & 0.9515 & 0.0077 \\\\\n",
      "full panel, R2:\n",
      ", full & 0.9421 & 0.9528 & 0.9052 & 0.0148 \\\\\n",
      "midjet panel, correlation:\n",
      ", mid & 0.9531 & 0.9629 & 0.9202 & 0.0132 \\\\\n",
      "midjet panel, R2:\n",
      ", mid & 0.9078 & 0.9264 & 0.8467 & 0.0248 \\\\\n"
     ]
    }
   ],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Change below for each Configuration ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "vi1 = 'ssh_ins'\n",
    "vi2 = 'T_xy_ins'\n",
    "vi3 = 'u_xy_ins'\n",
    "vi4 = 'v_xy_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}{}{}{}_{}{}_nobatchnorm'.format(vi1, vi2, vi3, vi4, vo1, vo2)\n",
    "var_input_names = [vi1, vi2, vi3, vi4]\n",
    "var_output_names = [vo1, vo2]\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "\n",
    "inp_test, out_test = loaddata_preloaded_test(all_test_input, all_test_output)\n",
    "out_test = out_test*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "truth_bot = out_test[:, :, bottom_slice, :]\n",
    "truth_mid = out_test[:, :, mid_slice, :]\n",
    "truth_top = out_test[:, :, top_slice, :]\n",
    "\n",
    "combined_names = ''.join(var_input_names)\n",
    "\n",
    "#Array to record performance metrics\n",
    "corr_ensemble_full = np.zeros(nensemble)\n",
    "R2_ensemble_full = np.zeros(nensemble)\n",
    "corr_ensemble_top = np.zeros(nensemble)\n",
    "R2_ensemble_top = np.zeros(nensemble)\n",
    "corr_ensemble_mid = np.zeros(nensemble)\n",
    "R2_ensemble_mid = np.zeros(nensemble)\n",
    "corr_ensemble_bot = np.zeros(nensemble)\n",
    "R2_ensemble_bot = np.zeros(nensemble)\n",
    "for iensemble in np.arange(nensemble):\n",
    "    model_filename = f'any_{combined_names}{filesuffix1}{iensemble}{filesuffix2}'\n",
    "    model_path = Path(model_folder, model_filename)\n",
    "    state_dict = torch.load(model_path)\n",
    "    # Create a new instance of the model\n",
    "    model = UNet_nobatchnorm(N_inp, N_out, bilinear = True, Nbase = Nbase)\n",
    "    # Load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_mod = model(totorch(inp_test)).detach().cpu().numpy()\n",
    "    #Renormalize\n",
    "    out_mod = out_mod*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "    mod_bot = out_mod[:, :, bottom_slice, :]\n",
    "    mod_mid = out_mod[:, :, mid_slice, :]\n",
    "    mod_top = out_mod[:, :, top_slice, :]\n",
    "\n",
    "    corr_ensemble_full[iensemble] = corr(out_test, out_mod)\n",
    "    R2_ensemble_full[iensemble] = L2_R(out_test, out_mod)    \n",
    "    corr_ensemble_top[iensemble] = corr(truth_top, mod_top)\n",
    "    R2_ensemble_top[iensemble] = L2_R(truth_top, mod_top)  \n",
    "    corr_ensemble_mid[iensemble] = corr(truth_mid, mod_mid)\n",
    "    R2_ensemble_mid[iensemble] = L2_R(truth_mid, mod_mid)\n",
    "    corr_ensemble_bot[iensemble] = corr(truth_bot, mod_bot)\n",
    "    R2_ensemble_bot[iensemble] = L2_R(truth_bot, mod_bot)\n",
    "\n",
    "print('full panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_full)\n",
    "max_val = np.max(corr_ensemble_full)\n",
    "min_val = np.min(corr_ensemble_full)\n",
    "std_val = np.std(corr_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('full panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_full)\n",
    "max_val = np.max(R2_ensemble_full)\n",
    "min_val = np.min(R2_ensemble_full)\n",
    "std_val = np.std(R2_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_mid)\n",
    "max_val = np.max(corr_ensemble_mid)\n",
    "min_val = np.min(corr_ensemble_mid)\n",
    "std_val = np.std(corr_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_mid)\n",
    "max_val = np.max(R2_ensemble_mid)\n",
    "min_val = np.min(R2_ensemble_mid)\n",
    "std_val = np.std(R2_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full panel, correlation:\n",
      ", full & 0.3970 & 0.4168 & 0.3700 & 0.0143 \\\\\n",
      "full panel, R2:\n",
      ", full & 0.1453 & 0.1649 & 0.1227 & 0.0137 \\\\\n",
      "midjet panel, correlation:\n",
      ", mid & 0.0442 & 0.0697 & 0.0066 & 0.0222 \\\\\n",
      "midjet panel, R2:\n",
      ", mid & -0.0491 & -0.0156 & -0.0899 & 0.0239 \\\\\n"
     ]
    }
   ],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Change below for each Configuration ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "vi1 = 'T_xy_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}_{}{}_nobatchnorm'.format(vi1, vo1, vo2)\n",
    "var_input_names = [vi1]\n",
    "var_output_names = [vo1, vo2]\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "\n",
    "inp_test, out_test = loaddata_preloaded_test(all_test_input, all_test_output)\n",
    "out_test = out_test*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "truth_bot = out_test[:, :, bottom_slice, :]\n",
    "truth_mid = out_test[:, :, mid_slice, :]\n",
    "truth_top = out_test[:, :, top_slice, :]\n",
    "\n",
    "combined_names = ''.join(var_input_names)\n",
    "\n",
    "#Array to record performance metrics\n",
    "corr_ensemble_full = np.zeros(nensemble)\n",
    "R2_ensemble_full = np.zeros(nensemble)\n",
    "corr_ensemble_top = np.zeros(nensemble)\n",
    "R2_ensemble_top = np.zeros(nensemble)\n",
    "corr_ensemble_mid = np.zeros(nensemble)\n",
    "R2_ensemble_mid = np.zeros(nensemble)\n",
    "corr_ensemble_bot = np.zeros(nensemble)\n",
    "R2_ensemble_bot = np.zeros(nensemble)\n",
    "for iensemble in np.arange(nensemble):\n",
    "    model_filename = f'any_{combined_names}{filesuffix1}{iensemble}{filesuffix2}'\n",
    "    model_path = Path(model_folder, model_filename)\n",
    "    state_dict = torch.load(model_path)\n",
    "    # Create a new instance of the model\n",
    "    model = UNet_nobatchnorm(N_inp, N_out, bilinear = True, Nbase = Nbase)\n",
    "    # Load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_mod = model(totorch(inp_test)).detach().cpu().numpy()\n",
    "    #Renormalize\n",
    "    out_mod = out_mod*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "    mod_bot = out_mod[:, :, bottom_slice, :]\n",
    "    mod_mid = out_mod[:, :, mid_slice, :]\n",
    "    mod_top = out_mod[:, :, top_slice, :]\n",
    "\n",
    "    corr_ensemble_full[iensemble] = corr(out_test, out_mod)\n",
    "    R2_ensemble_full[iensemble] = L2_R(out_test, out_mod)    \n",
    "    corr_ensemble_top[iensemble] = corr(truth_top, mod_top)\n",
    "    R2_ensemble_top[iensemble] = L2_R(truth_top, mod_top)  \n",
    "    corr_ensemble_mid[iensemble] = corr(truth_mid, mod_mid)\n",
    "    R2_ensemble_mid[iensemble] = L2_R(truth_mid, mod_mid)\n",
    "    corr_ensemble_bot[iensemble] = corr(truth_bot, mod_bot)\n",
    "    R2_ensemble_bot[iensemble] = L2_R(truth_bot, mod_bot)\n",
    "\n",
    "print('full panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_full)\n",
    "max_val = np.max(corr_ensemble_full)\n",
    "min_val = np.min(corr_ensemble_full)\n",
    "std_val = np.std(corr_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('full panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_full)\n",
    "max_val = np.max(R2_ensemble_full)\n",
    "min_val = np.min(R2_ensemble_full)\n",
    "std_val = np.std(R2_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_mid)\n",
    "max_val = np.max(corr_ensemble_mid)\n",
    "min_val = np.min(corr_ensemble_mid)\n",
    "std_val = np.std(corr_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_mid)\n",
    "max_val = np.max(R2_ensemble_mid)\n",
    "min_val = np.min(R2_ensemble_mid)\n",
    "std_val = np.std(R2_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full panel, correlation:\n",
      ", full & 0.9407 & 0.9415 & 0.9400 & 0.0006 \\\\\n",
      "full panel, R2:\n",
      ", full & 0.8848 & 0.8863 & 0.8835 & 0.0012 \\\\\n",
      "midjet panel, correlation:\n",
      ", mid & 0.9021 & 0.9038 & 0.8996 & 0.0012 \\\\\n",
      "midjet panel, R2:\n",
      ", mid & 0.8136 & 0.8165 & 0.8090 & 0.0022 \\\\\n"
     ]
    }
   ],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Change below for each Configuration ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "vi1 = 'u_xy_ins'\n",
    "vi2 = 'v_xy_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}{}_{}{}_nobatchnorm'.format(vi1, vi2, vo1, vo2)\n",
    "var_input_names = [vi1, vi2]\n",
    "var_output_names = [vo1, vo2]\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "\n",
    "inp_test, out_test = loaddata_preloaded_test(all_test_input, all_test_output)\n",
    "out_test = out_test*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "truth_bot = out_test[:, :, bottom_slice, :]\n",
    "truth_mid = out_test[:, :, mid_slice, :]\n",
    "truth_top = out_test[:, :, top_slice, :]\n",
    "\n",
    "combined_names = ''.join(var_input_names)\n",
    "\n",
    "#Array to record performance metrics\n",
    "corr_ensemble_full = np.zeros(nensemble)\n",
    "R2_ensemble_full = np.zeros(nensemble)\n",
    "corr_ensemble_top = np.zeros(nensemble)\n",
    "R2_ensemble_top = np.zeros(nensemble)\n",
    "corr_ensemble_mid = np.zeros(nensemble)\n",
    "R2_ensemble_mid = np.zeros(nensemble)\n",
    "corr_ensemble_bot = np.zeros(nensemble)\n",
    "R2_ensemble_bot = np.zeros(nensemble)\n",
    "for iensemble in np.arange(nensemble):\n",
    "    model_filename = f'any_{combined_names}{filesuffix1}{iensemble}{filesuffix2}'\n",
    "    model_path = Path(model_folder, model_filename)\n",
    "    state_dict = torch.load(model_path)\n",
    "    # Create a new instance of the model\n",
    "    model = UNet_nobatchnorm(N_inp, N_out, bilinear = True, Nbase = Nbase)\n",
    "    # Load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_mod = model(totorch(inp_test)).detach().cpu().numpy()\n",
    "    #Renormalize\n",
    "    out_mod = out_mod*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "    mod_bot = out_mod[:, :, bottom_slice, :]\n",
    "    mod_mid = out_mod[:, :, mid_slice, :]\n",
    "    mod_top = out_mod[:, :, top_slice, :]\n",
    "\n",
    "    corr_ensemble_full[iensemble] = corr(out_test, out_mod)\n",
    "    R2_ensemble_full[iensemble] = L2_R(out_test, out_mod)    \n",
    "    corr_ensemble_top[iensemble] = corr(truth_top, mod_top)\n",
    "    R2_ensemble_top[iensemble] = L2_R(truth_top, mod_top)  \n",
    "    corr_ensemble_mid[iensemble] = corr(truth_mid, mod_mid)\n",
    "    R2_ensemble_mid[iensemble] = L2_R(truth_mid, mod_mid)\n",
    "    corr_ensemble_bot[iensemble] = corr(truth_bot, mod_bot)\n",
    "    R2_ensemble_bot[iensemble] = L2_R(truth_bot, mod_bot)\n",
    "\n",
    "print('full panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_full)\n",
    "max_val = np.max(corr_ensemble_full)\n",
    "min_val = np.min(corr_ensemble_full)\n",
    "std_val = np.std(corr_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('full panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_full)\n",
    "max_val = np.max(R2_ensemble_full)\n",
    "min_val = np.min(R2_ensemble_full)\n",
    "std_val = np.std(R2_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_mid)\n",
    "max_val = np.max(corr_ensemble_mid)\n",
    "min_val = np.min(corr_ensemble_mid)\n",
    "std_val = np.std(corr_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_mid)\n",
    "max_val = np.max(R2_ensemble_mid)\n",
    "min_val = np.min(R2_ensemble_mid)\n",
    "std_val = np.std(R2_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full panel, correlation:\n",
      ", full & 0.8568 & 0.8619 & 0.8488 & 0.0038 \\\\\n",
      "full panel, R2:\n",
      ", full & 0.7328 & 0.7415 & 0.7194 & 0.0064 \\\\\n",
      "midjet panel, correlation:\n",
      ", mid & 0.7902 & 0.8015 & 0.7783 & 0.0067 \\\\\n",
      "midjet panel, R2:\n",
      ", mid & 0.6235 & 0.6416 & 0.6052 & 0.0107 \\\\\n"
     ]
    }
   ],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Change below for each Configuration ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "vi1 = 'ssh_ins'\n",
    "vi2 = 'T_xy_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}{}_{}{}_nobatchnorm'.format(vi1, vi2, vo1, vo2)\n",
    "var_input_names = [vi1, vi2]\n",
    "var_output_names = [vo1, vo2]\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "\n",
    "inp_test, out_test = loaddata_preloaded_test(all_test_input, all_test_output)\n",
    "out_test = out_test*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "truth_bot = out_test[:, :, bottom_slice, :]\n",
    "truth_mid = out_test[:, :, mid_slice, :]\n",
    "truth_top = out_test[:, :, top_slice, :]\n",
    "\n",
    "combined_names = ''.join(var_input_names)\n",
    "\n",
    "#Array to record performance metrics\n",
    "corr_ensemble_full = np.zeros(nensemble)\n",
    "R2_ensemble_full = np.zeros(nensemble)\n",
    "corr_ensemble_top = np.zeros(nensemble)\n",
    "R2_ensemble_top = np.zeros(nensemble)\n",
    "corr_ensemble_mid = np.zeros(nensemble)\n",
    "R2_ensemble_mid = np.zeros(nensemble)\n",
    "corr_ensemble_bot = np.zeros(nensemble)\n",
    "R2_ensemble_bot = np.zeros(nensemble)\n",
    "for iensemble in np.arange(nensemble):\n",
    "    model_filename = f'any_{combined_names}{filesuffix1}{iensemble}{filesuffix2}'\n",
    "    model_path = Path(model_folder, model_filename)\n",
    "    state_dict = torch.load(model_path)\n",
    "    # Create a new instance of the model\n",
    "    model = UNet_nobatchnorm(N_inp, N_out, bilinear = True, Nbase = Nbase)\n",
    "    # Load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_mod = model(totorch(inp_test)).detach().cpu().numpy()\n",
    "    #Renormalize\n",
    "    out_mod = out_mod*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "    mod_bot = out_mod[:, :, bottom_slice, :]\n",
    "    mod_mid = out_mod[:, :, mid_slice, :]\n",
    "    mod_top = out_mod[:, :, top_slice, :]\n",
    "\n",
    "    corr_ensemble_full[iensemble] = corr(out_test, out_mod)\n",
    "    R2_ensemble_full[iensemble] = L2_R(out_test, out_mod)    \n",
    "    corr_ensemble_top[iensemble] = corr(truth_top, mod_top)\n",
    "    R2_ensemble_top[iensemble] = L2_R(truth_top, mod_top)  \n",
    "    corr_ensemble_mid[iensemble] = corr(truth_mid, mod_mid)\n",
    "    R2_ensemble_mid[iensemble] = L2_R(truth_mid, mod_mid)\n",
    "    corr_ensemble_bot[iensemble] = corr(truth_bot, mod_bot)\n",
    "    R2_ensemble_bot[iensemble] = L2_R(truth_bot, mod_bot)\n",
    "\n",
    "print('full panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_full)\n",
    "max_val = np.max(corr_ensemble_full)\n",
    "min_val = np.min(corr_ensemble_full)\n",
    "std_val = np.std(corr_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('full panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_full)\n",
    "max_val = np.max(R2_ensemble_full)\n",
    "min_val = np.min(R2_ensemble_full)\n",
    "std_val = np.std(R2_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_mid)\n",
    "max_val = np.max(corr_ensemble_mid)\n",
    "min_val = np.min(corr_ensemble_mid)\n",
    "std_val = np.std(corr_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_mid)\n",
    "max_val = np.max(R2_ensemble_mid)\n",
    "min_val = np.min(R2_ensemble_mid)\n",
    "std_val = np.std(R2_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full panel, correlation:\n",
      ", full & 0.9627 & 0.9744 & 0.9529 & 0.0079 \\\\\n",
      "full panel, R2:\n",
      ", full & 0.9267 & 0.9493 & 0.9078 & 0.0152 \\\\\n",
      "midjet panel, correlation:\n",
      ", mid & 0.9384 & 0.9594 & 0.9218 & 0.0141 \\\\\n",
      "midjet panel, R2:\n",
      ", mid & 0.8804 & 0.9202 & 0.8493 & 0.0263 \\\\\n"
     ]
    }
   ],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Change below for each Configuration ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "vi1 = 'ssh_ins'\n",
    "vi2 = 'u_xy_ins'\n",
    "vi3 = 'v_xy_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}{}{}_{}{}_nobatchnorm'.format(vi1, vi2, vi3, vo1, vo2)\n",
    "var_input_names = [vi1, vi2, vi3]\n",
    "var_output_names = [vo1, vo2]\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "\n",
    "inp_test, out_test = loaddata_preloaded_test(all_test_input, all_test_output)\n",
    "out_test = out_test*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "truth_bot = out_test[:, :, bottom_slice, :]\n",
    "truth_mid = out_test[:, :, mid_slice, :]\n",
    "truth_top = out_test[:, :, top_slice, :]\n",
    "\n",
    "combined_names = ''.join(var_input_names)\n",
    "\n",
    "#Array to record performance metrics\n",
    "corr_ensemble_full = np.zeros(nensemble)\n",
    "R2_ensemble_full = np.zeros(nensemble)\n",
    "corr_ensemble_top = np.zeros(nensemble)\n",
    "R2_ensemble_top = np.zeros(nensemble)\n",
    "corr_ensemble_mid = np.zeros(nensemble)\n",
    "R2_ensemble_mid = np.zeros(nensemble)\n",
    "corr_ensemble_bot = np.zeros(nensemble)\n",
    "R2_ensemble_bot = np.zeros(nensemble)\n",
    "for iensemble in np.arange(nensemble):\n",
    "    model_filename = f'any_{combined_names}{filesuffix1}{iensemble}{filesuffix2}'\n",
    "    model_path = Path(model_folder, model_filename)\n",
    "    state_dict = torch.load(model_path)\n",
    "    # Create a new instance of the model\n",
    "    model = UNet_nobatchnorm(N_inp, N_out, bilinear = True, Nbase = Nbase)\n",
    "    # Load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_mod = model(totorch(inp_test)).detach().cpu().numpy()\n",
    "    #Renormalize\n",
    "    out_mod = out_mod*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "    mod_bot = out_mod[:, :, bottom_slice, :]\n",
    "    mod_mid = out_mod[:, :, mid_slice, :]\n",
    "    mod_top = out_mod[:, :, top_slice, :]\n",
    "\n",
    "    corr_ensemble_full[iensemble] = corr(out_test, out_mod)\n",
    "    R2_ensemble_full[iensemble] = L2_R(out_test, out_mod)    \n",
    "    corr_ensemble_top[iensemble] = corr(truth_top, mod_top)\n",
    "    R2_ensemble_top[iensemble] = L2_R(truth_top, mod_top)  \n",
    "    corr_ensemble_mid[iensemble] = corr(truth_mid, mod_mid)\n",
    "    R2_ensemble_mid[iensemble] = L2_R(truth_mid, mod_mid)\n",
    "    corr_ensemble_bot[iensemble] = corr(truth_bot, mod_bot)\n",
    "    R2_ensemble_bot[iensemble] = L2_R(truth_bot, mod_bot)\n",
    "\n",
    "print('full panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_full)\n",
    "max_val = np.max(corr_ensemble_full)\n",
    "min_val = np.min(corr_ensemble_full)\n",
    "std_val = np.std(corr_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('full panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_full)\n",
    "max_val = np.max(R2_ensemble_full)\n",
    "min_val = np.min(R2_ensemble_full)\n",
    "std_val = np.std(R2_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_mid)\n",
    "max_val = np.max(corr_ensemble_mid)\n",
    "min_val = np.min(corr_ensemble_mid)\n",
    "std_val = np.std(corr_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_mid)\n",
    "max_val = np.max(R2_ensemble_mid)\n",
    "min_val = np.min(R2_ensemble_mid)\n",
    "std_val = np.std(R2_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full panel, correlation:\n",
      ", full & 0.9522 & 0.9534 & 0.9507 & 0.0008 \\\\\n",
      "full panel, R2:\n",
      ", full & 0.9065 & 0.9087 & 0.9034 & 0.0016 \\\\\n",
      "midjet panel, correlation:\n",
      ", mid & 0.9198 & 0.9217 & 0.9178 & 0.0014 \\\\\n",
      "midjet panel, R2:\n",
      ", mid & 0.8458 & 0.8495 & 0.8422 & 0.0025 \\\\\n"
     ]
    }
   ],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Change below for each Configuration ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "vi1 = 'T_xy_ins'\n",
    "vi2 = 'u_xy_ins'\n",
    "vi3 = 'v_xy_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}{}{}_{}{}_nobatchnorm'.format(vi1, vi2, vi3, vo1, vo2)\n",
    "var_input_names = [vi1, vi2, vi3]\n",
    "var_output_names = [vo1, vo2]\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "\n",
    "inp_test, out_test = loaddata_preloaded_test(all_test_input, all_test_output)\n",
    "out_test = out_test*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "truth_bot = out_test[:, :, bottom_slice, :]\n",
    "truth_mid = out_test[:, :, mid_slice, :]\n",
    "truth_top = out_test[:, :, top_slice, :]\n",
    "\n",
    "combined_names = ''.join(var_input_names)\n",
    "\n",
    "#Array to record performance metrics\n",
    "corr_ensemble_full = np.zeros(nensemble)\n",
    "R2_ensemble_full = np.zeros(nensemble)\n",
    "corr_ensemble_top = np.zeros(nensemble)\n",
    "R2_ensemble_top = np.zeros(nensemble)\n",
    "corr_ensemble_mid = np.zeros(nensemble)\n",
    "R2_ensemble_mid = np.zeros(nensemble)\n",
    "corr_ensemble_bot = np.zeros(nensemble)\n",
    "R2_ensemble_bot = np.zeros(nensemble)\n",
    "for iensemble in np.arange(nensemble):\n",
    "    model_filename = f'any_{combined_names}{filesuffix1}{iensemble}{filesuffix2}'\n",
    "    model_path = Path(model_folder, model_filename)\n",
    "    state_dict = torch.load(model_path)\n",
    "    # Create a new instance of the model\n",
    "    model = UNet_nobatchnorm(N_inp, N_out, bilinear = True, Nbase = Nbase)\n",
    "    # Load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_mod = model(totorch(inp_test)).detach().cpu().numpy()\n",
    "    #Renormalize\n",
    "    out_mod = out_mod*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "    mod_bot = out_mod[:, :, bottom_slice, :]\n",
    "    mod_mid = out_mod[:, :, mid_slice, :]\n",
    "    mod_top = out_mod[:, :, top_slice, :]\n",
    "\n",
    "    corr_ensemble_full[iensemble] = corr(out_test, out_mod)\n",
    "    R2_ensemble_full[iensemble] = L2_R(out_test, out_mod)    \n",
    "    corr_ensemble_top[iensemble] = corr(truth_top, mod_top)\n",
    "    R2_ensemble_top[iensemble] = L2_R(truth_top, mod_top)  \n",
    "    corr_ensemble_mid[iensemble] = corr(truth_mid, mod_mid)\n",
    "    R2_ensemble_mid[iensemble] = L2_R(truth_mid, mod_mid)\n",
    "    corr_ensemble_bot[iensemble] = corr(truth_bot, mod_bot)\n",
    "    R2_ensemble_bot[iensemble] = L2_R(truth_bot, mod_bot)\n",
    "\n",
    "print('full panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_full)\n",
    "max_val = np.max(corr_ensemble_full)\n",
    "min_val = np.min(corr_ensemble_full)\n",
    "std_val = np.std(corr_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('full panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_full)\n",
    "max_val = np.max(R2_ensemble_full)\n",
    "min_val = np.min(R2_ensemble_full)\n",
    "std_val = np.std(R2_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_mid)\n",
    "max_val = np.max(corr_ensemble_mid)\n",
    "min_val = np.min(corr_ensemble_mid)\n",
    "std_val = np.std(corr_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_mid)\n",
    "max_val = np.max(R2_ensemble_mid)\n",
    "min_val = np.min(R2_ensemble_mid)\n",
    "std_val = np.std(R2_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below are to check the runs with vorticity and divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full panel, correlation:\n",
      ", full & 0.8201 & 0.8258 & 0.8151 & 0.0031 \\\\\n",
      "full panel, R2:\n",
      ", full & 0.6719 & 0.6815 & 0.6642 & 0.0051 \\\\\n",
      "midjet panel, correlation:\n",
      ", mid & 0.7125 & 0.7224 & 0.7049 & 0.0049 \\\\\n",
      "midjet panel, R2:\n",
      ", mid & 0.5072 & 0.5209 & 0.4969 & 0.0069 \\\\\n"
     ]
    }
   ],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Change below for each Configuration ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "vi1 = 'vort'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}_{}{}_nobatchnorm'.format(vi1, vo1, vo2)\n",
    "var_input_names = [vi1]\n",
    "var_output_names = [vo1, vo2]\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data_vortdiv(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data_vortdiv(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "\n",
    "inp_test, out_test = loaddata_preloaded_test(all_test_input, all_test_output)\n",
    "out_test = out_test*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "truth_bot = out_test[:, :, bottom_slice, :]\n",
    "truth_mid = out_test[:, :, mid_slice, :]\n",
    "truth_top = out_test[:, :, top_slice, :]\n",
    "\n",
    "combined_names = ''.join(var_input_names)\n",
    "\n",
    "#Array to record performance metrics\n",
    "corr_ensemble_full = np.zeros(nensemble)\n",
    "R2_ensemble_full = np.zeros(nensemble)\n",
    "corr_ensemble_top = np.zeros(nensemble)\n",
    "R2_ensemble_top = np.zeros(nensemble)\n",
    "corr_ensemble_mid = np.zeros(nensemble)\n",
    "R2_ensemble_mid = np.zeros(nensemble)\n",
    "corr_ensemble_bot = np.zeros(nensemble)\n",
    "R2_ensemble_bot = np.zeros(nensemble)\n",
    "for iensemble in np.arange(nensemble):\n",
    "    model_filename = f'any_{combined_names}{filesuffix1}{iensemble}{filesuffix2}'\n",
    "    model_path = Path(model_folder, model_filename)\n",
    "    state_dict = torch.load(model_path)\n",
    "    # Create a new instance of the model\n",
    "    model = UNet_nobatchnorm(N_inp, N_out, bilinear = True, Nbase = Nbase)\n",
    "    # Load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_mod = model(totorch(inp_test)).detach().cpu().numpy()\n",
    "    #Renormalize\n",
    "    out_mod = out_mod*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "    mod_bot = out_mod[:, :, bottom_slice, :]\n",
    "    mod_mid = out_mod[:, :, mid_slice, :]\n",
    "    mod_top = out_mod[:, :, top_slice, :]\n",
    "\n",
    "    corr_ensemble_full[iensemble] = corr(out_test, out_mod)\n",
    "    R2_ensemble_full[iensemble] = L2_R(out_test, out_mod)    \n",
    "    corr_ensemble_top[iensemble] = corr(truth_top, mod_top)\n",
    "    R2_ensemble_top[iensemble] = L2_R(truth_top, mod_top)  \n",
    "    corr_ensemble_mid[iensemble] = corr(truth_mid, mod_mid)\n",
    "    R2_ensemble_mid[iensemble] = L2_R(truth_mid, mod_mid)\n",
    "    corr_ensemble_bot[iensemble] = corr(truth_bot, mod_bot)\n",
    "    R2_ensemble_bot[iensemble] = L2_R(truth_bot, mod_bot)\n",
    "\n",
    "print('full panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_full)\n",
    "max_val = np.max(corr_ensemble_full)\n",
    "min_val = np.min(corr_ensemble_full)\n",
    "std_val = np.std(corr_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('full panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_full)\n",
    "max_val = np.max(R2_ensemble_full)\n",
    "min_val = np.min(R2_ensemble_full)\n",
    "std_val = np.std(R2_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_mid)\n",
    "max_val = np.max(corr_ensemble_mid)\n",
    "min_val = np.min(corr_ensemble_mid)\n",
    "std_val = np.std(corr_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_mid)\n",
    "max_val = np.max(R2_ensemble_mid)\n",
    "min_val = np.min(R2_ensemble_mid)\n",
    "std_val = np.std(R2_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full panel, correlation:\n",
      ", full & 0.8470 & 0.8501 & 0.8437 & 0.0019 \\\\\n",
      "full panel, R2:\n",
      ", full & 0.7171 & 0.7221 & 0.7119 & 0.0031 \\\\\n",
      "midjet panel, correlation:\n",
      ", mid & 0.7549 & 0.7637 & 0.7470 & 0.0049 \\\\\n",
      "midjet panel, R2:\n",
      ", mid & 0.5696 & 0.5832 & 0.5577 & 0.0075 \\\\\n"
     ]
    }
   ],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Change below for each Configuration ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "vi1 = 'vort'\n",
    "vi2 = 'T_xy_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}{}_{}{}_nobatchnorm'.format(vi1, vi2, vo1, vo2)\n",
    "var_input_names = [vi1, vi2]\n",
    "var_output_names = [vo1, vo2]\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data_vortdiv(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data_vortdiv(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "\n",
    "inp_test, out_test = loaddata_preloaded_test(all_test_input, all_test_output)\n",
    "out_test = out_test*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "truth_bot = out_test[:, :, bottom_slice, :]\n",
    "truth_mid = out_test[:, :, mid_slice, :]\n",
    "truth_top = out_test[:, :, top_slice, :]\n",
    "\n",
    "combined_names = ''.join(var_input_names)\n",
    "\n",
    "#Array to record performance metrics\n",
    "corr_ensemble_full = np.zeros(nensemble)\n",
    "R2_ensemble_full = np.zeros(nensemble)\n",
    "corr_ensemble_top = np.zeros(nensemble)\n",
    "R2_ensemble_top = np.zeros(nensemble)\n",
    "corr_ensemble_mid = np.zeros(nensemble)\n",
    "R2_ensemble_mid = np.zeros(nensemble)\n",
    "corr_ensemble_bot = np.zeros(nensemble)\n",
    "R2_ensemble_bot = np.zeros(nensemble)\n",
    "for iensemble in np.arange(nensemble):\n",
    "    model_filename = f'any_{combined_names}{filesuffix1}{iensemble}{filesuffix2}'\n",
    "    model_path = Path(model_folder, model_filename)\n",
    "    state_dict = torch.load(model_path)\n",
    "    # Create a new instance of the model\n",
    "    model = UNet_nobatchnorm(N_inp, N_out, bilinear = True, Nbase = Nbase)\n",
    "    # Load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_mod = model(totorch(inp_test)).detach().cpu().numpy()\n",
    "    #Renormalize\n",
    "    out_mod = out_mod*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "    mod_bot = out_mod[:, :, bottom_slice, :]\n",
    "    mod_mid = out_mod[:, :, mid_slice, :]\n",
    "    mod_top = out_mod[:, :, top_slice, :]\n",
    "\n",
    "    corr_ensemble_full[iensemble] = corr(out_test, out_mod)\n",
    "    R2_ensemble_full[iensemble] = L2_R(out_test, out_mod)    \n",
    "    corr_ensemble_top[iensemble] = corr(truth_top, mod_top)\n",
    "    R2_ensemble_top[iensemble] = L2_R(truth_top, mod_top)  \n",
    "    corr_ensemble_mid[iensemble] = corr(truth_mid, mod_mid)\n",
    "    R2_ensemble_mid[iensemble] = L2_R(truth_mid, mod_mid)\n",
    "    corr_ensemble_bot[iensemble] = corr(truth_bot, mod_bot)\n",
    "    R2_ensemble_bot[iensemble] = L2_R(truth_bot, mod_bot)\n",
    "\n",
    "print('full panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_full)\n",
    "max_val = np.max(corr_ensemble_full)\n",
    "min_val = np.min(corr_ensemble_full)\n",
    "std_val = np.std(corr_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('full panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_full)\n",
    "max_val = np.max(R2_ensemble_full)\n",
    "min_val = np.min(R2_ensemble_full)\n",
    "std_val = np.std(R2_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_mid)\n",
    "max_val = np.max(corr_ensemble_mid)\n",
    "min_val = np.min(corr_ensemble_mid)\n",
    "std_val = np.std(corr_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_mid)\n",
    "max_val = np.max(R2_ensemble_mid)\n",
    "min_val = np.min(R2_ensemble_mid)\n",
    "std_val = np.std(R2_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full panel, correlation:\n",
      ", full & 0.8792 & 0.8822 & 0.8735 & 0.0022 \\\\\n",
      "full panel, R2:\n",
      ", full & 0.7713 & 0.7763 & 0.7611 & 0.0039 \\\\\n",
      "midjet panel, correlation:\n",
      ", mid & 0.8239 & 0.8294 & 0.8158 & 0.0035 \\\\\n",
      "midjet panel, R2:\n",
      ", mid & 0.6782 & 0.6870 & 0.6649 & 0.0058 \\\\\n"
     ]
    }
   ],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Change below for each Configuration ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "vi1 = 'div'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}_{}{}_nobatchnorm'.format(vi1, vo1, vo2)\n",
    "var_input_names = [vi1]\n",
    "var_output_names = [vo1, vo2]\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data_vortdiv(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data_vortdiv(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "\n",
    "inp_test, out_test = loaddata_preloaded_test(all_test_input, all_test_output)\n",
    "out_test = out_test*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "truth_bot = out_test[:, :, bottom_slice, :]\n",
    "truth_mid = out_test[:, :, mid_slice, :]\n",
    "truth_top = out_test[:, :, top_slice, :]\n",
    "\n",
    "combined_names = ''.join(var_input_names)\n",
    "\n",
    "#Array to record performance metrics\n",
    "corr_ensemble_full = np.zeros(nensemble)\n",
    "R2_ensemble_full = np.zeros(nensemble)\n",
    "corr_ensemble_top = np.zeros(nensemble)\n",
    "R2_ensemble_top = np.zeros(nensemble)\n",
    "corr_ensemble_mid = np.zeros(nensemble)\n",
    "R2_ensemble_mid = np.zeros(nensemble)\n",
    "corr_ensemble_bot = np.zeros(nensemble)\n",
    "R2_ensemble_bot = np.zeros(nensemble)\n",
    "for iensemble in np.arange(nensemble):\n",
    "    model_filename = f'any_{combined_names}{filesuffix1}{iensemble}{filesuffix2}'\n",
    "    model_path = Path(model_folder, model_filename)\n",
    "    state_dict = torch.load(model_path)\n",
    "    # Create a new instance of the model\n",
    "    model = UNet_nobatchnorm(N_inp, N_out, bilinear = True, Nbase = Nbase)\n",
    "    # Load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_mod = model(totorch(inp_test)).detach().cpu().numpy()\n",
    "    #Renormalize\n",
    "    out_mod = out_mod*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "    mod_bot = out_mod[:, :, bottom_slice, :]\n",
    "    mod_mid = out_mod[:, :, mid_slice, :]\n",
    "    mod_top = out_mod[:, :, top_slice, :]\n",
    "\n",
    "    corr_ensemble_full[iensemble] = corr(out_test, out_mod)\n",
    "    R2_ensemble_full[iensemble] = L2_R(out_test, out_mod)    \n",
    "    corr_ensemble_top[iensemble] = corr(truth_top, mod_top)\n",
    "    R2_ensemble_top[iensemble] = L2_R(truth_top, mod_top)  \n",
    "    corr_ensemble_mid[iensemble] = corr(truth_mid, mod_mid)\n",
    "    R2_ensemble_mid[iensemble] = L2_R(truth_mid, mod_mid)\n",
    "    corr_ensemble_bot[iensemble] = corr(truth_bot, mod_bot)\n",
    "    R2_ensemble_bot[iensemble] = L2_R(truth_bot, mod_bot)\n",
    "\n",
    "print('full panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_full)\n",
    "max_val = np.max(corr_ensemble_full)\n",
    "min_val = np.min(corr_ensemble_full)\n",
    "std_val = np.std(corr_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('full panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_full)\n",
    "max_val = np.max(R2_ensemble_full)\n",
    "min_val = np.min(R2_ensemble_full)\n",
    "std_val = np.std(R2_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_mid)\n",
    "max_val = np.max(corr_ensemble_mid)\n",
    "min_val = np.min(corr_ensemble_mid)\n",
    "std_val = np.std(corr_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_mid)\n",
    "max_val = np.max(R2_ensemble_mid)\n",
    "min_val = np.min(R2_ensemble_mid)\n",
    "std_val = np.std(R2_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full panel, correlation:\n",
      ", full & 0.9294 & 0.9300 & 0.9286 & 0.0005 \\\\\n",
      "full panel, R2:\n",
      ", full & 0.8633 & 0.8644 & 0.8611 & 0.0011 \\\\\n",
      "midjet panel, correlation:\n",
      ", mid & 0.8811 & 0.8826 & 0.8794 & 0.0008 \\\\\n",
      "midjet panel, R2:\n",
      ", mid & 0.7757 & 0.7787 & 0.7727 & 0.0016 \\\\\n"
     ]
    }
   ],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Change below for each Configuration ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "vi1 = 'vort'\n",
    "vi2 = 'div'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}{}_{}{}_nobatchnorm'.format(vi1, vi2, vo1, vo2)\n",
    "var_input_names = [vi1, vi2]\n",
    "var_output_names = [vo1, vo2]\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data_vortdiv(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data_vortdiv(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "\n",
    "inp_test, out_test = loaddata_preloaded_test(all_test_input, all_test_output)\n",
    "out_test = out_test*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "truth_bot = out_test[:, :, bottom_slice, :]\n",
    "truth_mid = out_test[:, :, mid_slice, :]\n",
    "truth_top = out_test[:, :, top_slice, :]\n",
    "\n",
    "combined_names = ''.join(var_input_names)\n",
    "\n",
    "#Array to record performance metrics\n",
    "corr_ensemble_full = np.zeros(nensemble)\n",
    "R2_ensemble_full = np.zeros(nensemble)\n",
    "corr_ensemble_top = np.zeros(nensemble)\n",
    "R2_ensemble_top = np.zeros(nensemble)\n",
    "corr_ensemble_mid = np.zeros(nensemble)\n",
    "R2_ensemble_mid = np.zeros(nensemble)\n",
    "corr_ensemble_bot = np.zeros(nensemble)\n",
    "R2_ensemble_bot = np.zeros(nensemble)\n",
    "for iensemble in np.arange(nensemble):\n",
    "    model_filename = f'any_{combined_names}{filesuffix1}{iensemble}{filesuffix2}'\n",
    "    model_path = Path(model_folder, model_filename)\n",
    "    state_dict = torch.load(model_path)\n",
    "    # Create a new instance of the model\n",
    "    model = UNet_nobatchnorm(N_inp, N_out, bilinear = True, Nbase = Nbase)\n",
    "    # Load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_mod = model(totorch(inp_test)).detach().cpu().numpy()\n",
    "    #Renormalize\n",
    "    out_mod = out_mod*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "    mod_bot = out_mod[:, :, bottom_slice, :]\n",
    "    mod_mid = out_mod[:, :, mid_slice, :]\n",
    "    mod_top = out_mod[:, :, top_slice, :]\n",
    "\n",
    "    corr_ensemble_full[iensemble] = corr(out_test, out_mod)\n",
    "    R2_ensemble_full[iensemble] = L2_R(out_test, out_mod)    \n",
    "    corr_ensemble_top[iensemble] = corr(truth_top, mod_top)\n",
    "    R2_ensemble_top[iensemble] = L2_R(truth_top, mod_top)  \n",
    "    corr_ensemble_mid[iensemble] = corr(truth_mid, mod_mid)\n",
    "    R2_ensemble_mid[iensemble] = L2_R(truth_mid, mod_mid)\n",
    "    corr_ensemble_bot[iensemble] = corr(truth_bot, mod_bot)\n",
    "    R2_ensemble_bot[iensemble] = L2_R(truth_bot, mod_bot)\n",
    "\n",
    "print('full panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_full)\n",
    "max_val = np.max(corr_ensemble_full)\n",
    "min_val = np.min(corr_ensemble_full)\n",
    "std_val = np.std(corr_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('full panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_full)\n",
    "max_val = np.max(R2_ensemble_full)\n",
    "min_val = np.min(R2_ensemble_full)\n",
    "std_val = np.std(R2_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_mid)\n",
    "max_val = np.max(corr_ensemble_mid)\n",
    "min_val = np.min(corr_ensemble_mid)\n",
    "std_val = np.std(corr_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_mid)\n",
    "max_val = np.max(R2_ensemble_mid)\n",
    "min_val = np.min(R2_ensemble_mid)\n",
    "std_val = np.std(R2_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4128800/2542605317.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2542605317.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2542605317.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2542605317.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2542605317.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2542605317.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2542605317.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2542605317.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2542605317.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2542605317.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full panel, correlation:\n",
      ", full & 0.8151 & 0.8189 & 0.8090 & 0.0027 \\\\\n",
      "full panel, R2:\n",
      ", full & 0.6627 & 0.6696 & 0.6521 & 0.0047 \\\\\n",
      "midjet panel, correlation:\n",
      ", mid & 0.6946 & 0.7030 & 0.6847 & 0.0052 \\\\\n",
      "midjet panel, R2:\n",
      ", mid & 0.4800 & 0.4932 & 0.4646 & 0.0080 \\\\\n"
     ]
    }
   ],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Change below for each Configuration ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "vi1 = 'u_xy_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}_{}{}_nobatchnorm'.format(vi1, vo1, vo2)\n",
    "var_input_names = [vi1]\n",
    "var_output_names = [vo1, vo2]\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "\n",
    "inp_test, out_test = loaddata_preloaded_test(all_test_input, all_test_output)\n",
    "out_test = out_test*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "truth_bot = out_test[:, :, bottom_slice, :]\n",
    "truth_mid = out_test[:, :, mid_slice, :]\n",
    "truth_top = out_test[:, :, top_slice, :]\n",
    "\n",
    "combined_names = ''.join(var_input_names)\n",
    "\n",
    "#Array to record performance metrics\n",
    "corr_ensemble_full = np.zeros(nensemble)\n",
    "R2_ensemble_full = np.zeros(nensemble)\n",
    "corr_ensemble_top = np.zeros(nensemble)\n",
    "R2_ensemble_top = np.zeros(nensemble)\n",
    "corr_ensemble_mid = np.zeros(nensemble)\n",
    "R2_ensemble_mid = np.zeros(nensemble)\n",
    "corr_ensemble_bot = np.zeros(nensemble)\n",
    "R2_ensemble_bot = np.zeros(nensemble)\n",
    "for iensemble in np.arange(nensemble):\n",
    "    model_filename = f'any_{combined_names}{filesuffix1}{iensemble}{filesuffix2}'\n",
    "    model_path = Path(model_folder, model_filename)\n",
    "    state_dict = torch.load(model_path)\n",
    "    # Create a new instance of the model\n",
    "    model = UNet_nobatchnorm(N_inp, N_out, bilinear = True, Nbase = Nbase)\n",
    "    # Load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_mod = model(totorch(inp_test)).detach().cpu().numpy()\n",
    "    #Renormalize\n",
    "    out_mod = out_mod*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "    mod_bot = out_mod[:, :, bottom_slice, :]\n",
    "    mod_mid = out_mod[:, :, mid_slice, :]\n",
    "    mod_top = out_mod[:, :, top_slice, :]\n",
    "\n",
    "    corr_ensemble_full[iensemble] = corr(out_test, out_mod)\n",
    "    R2_ensemble_full[iensemble] = L2_R(out_test, out_mod)    \n",
    "    corr_ensemble_top[iensemble] = corr(truth_top, mod_top)\n",
    "    R2_ensemble_top[iensemble] = L2_R(truth_top, mod_top)  \n",
    "    corr_ensemble_mid[iensemble] = corr(truth_mid, mod_mid)\n",
    "    R2_ensemble_mid[iensemble] = L2_R(truth_mid, mod_mid)\n",
    "    corr_ensemble_bot[iensemble] = corr(truth_bot, mod_bot)\n",
    "    R2_ensemble_bot[iensemble] = L2_R(truth_bot, mod_bot)\n",
    "\n",
    "print('full panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_full)\n",
    "max_val = np.max(corr_ensemble_full)\n",
    "min_val = np.min(corr_ensemble_full)\n",
    "std_val = np.std(corr_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('full panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_full)\n",
    "max_val = np.max(R2_ensemble_full)\n",
    "min_val = np.min(R2_ensemble_full)\n",
    "std_val = np.std(R2_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_mid)\n",
    "max_val = np.max(corr_ensemble_mid)\n",
    "min_val = np.min(corr_ensemble_mid)\n",
    "std_val = np.std(corr_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_mid)\n",
    "max_val = np.max(R2_ensemble_mid)\n",
    "min_val = np.min(R2_ensemble_mid)\n",
    "std_val = np.std(R2_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4128800/2761056495.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2761056495.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2761056495.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2761056495.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2761056495.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2761056495.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2761056495.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2761056495.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2761056495.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n",
      "/tmp/ipykernel_4128800/2761056495.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full panel, correlation:\n",
      ", full & 0.8560 & 0.8579 & 0.8542 & 0.0011 \\\\\n",
      "full panel, R2:\n",
      ", full & 0.7320 & 0.7351 & 0.7290 & 0.0019 \\\\\n",
      "midjet panel, correlation:\n",
      ", mid & 0.7768 & 0.7798 & 0.7740 & 0.0019 \\\\\n",
      "midjet panel, R2:\n",
      ", mid & 0.6030 & 0.6074 & 0.5990 & 0.0028 \\\\\n"
     ]
    }
   ],
   "source": [
    "# ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Change below for each Configuration ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "vi1 = 'v_xy_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}_{}{}_nobatchnorm'.format(vi1, vo1, vo2)\n",
    "var_input_names = [vi1]\n",
    "var_output_names = [vo1, vo2]\n",
    "# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "\n",
    "inp_test, out_test = loaddata_preloaded_test(all_test_input, all_test_output)\n",
    "out_test = out_test*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "truth_bot = out_test[:, :, bottom_slice, :]\n",
    "truth_mid = out_test[:, :, mid_slice, :]\n",
    "truth_top = out_test[:, :, top_slice, :]\n",
    "\n",
    "combined_names = ''.join(var_input_names)\n",
    "\n",
    "#Array to record performance metrics\n",
    "corr_ensemble_full = np.zeros(nensemble)\n",
    "R2_ensemble_full = np.zeros(nensemble)\n",
    "corr_ensemble_top = np.zeros(nensemble)\n",
    "R2_ensemble_top = np.zeros(nensemble)\n",
    "corr_ensemble_mid = np.zeros(nensemble)\n",
    "R2_ensemble_mid = np.zeros(nensemble)\n",
    "corr_ensemble_bot = np.zeros(nensemble)\n",
    "R2_ensemble_bot = np.zeros(nensemble)\n",
    "for iensemble in np.arange(nensemble):\n",
    "    model_filename = f'any_{combined_names}{filesuffix1}{iensemble}{filesuffix2}'\n",
    "    model_path = Path(model_folder, model_filename)\n",
    "    state_dict = torch.load(model_path)\n",
    "    # Create a new instance of the model\n",
    "    model = UNet_nobatchnorm(N_inp, N_out, bilinear = True, Nbase = Nbase)\n",
    "    # Load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out_mod = model(totorch(inp_test)).detach().cpu().numpy()\n",
    "    #Renormalize\n",
    "    out_mod = out_mod*np.sqrt(var_output[None, :, None, None])+mean_output[None, :, None, None]\n",
    "\n",
    "    mod_bot = out_mod[:, :, bottom_slice, :]\n",
    "    mod_mid = out_mod[:, :, mid_slice, :]\n",
    "    mod_top = out_mod[:, :, top_slice, :]\n",
    "\n",
    "    corr_ensemble_full[iensemble] = corr(out_test, out_mod)\n",
    "    R2_ensemble_full[iensemble] = L2_R(out_test, out_mod)    \n",
    "    corr_ensemble_top[iensemble] = corr(truth_top, mod_top)\n",
    "    R2_ensemble_top[iensemble] = L2_R(truth_top, mod_top)  \n",
    "    corr_ensemble_mid[iensemble] = corr(truth_mid, mod_mid)\n",
    "    R2_ensemble_mid[iensemble] = L2_R(truth_mid, mod_mid)\n",
    "    corr_ensemble_bot[iensemble] = corr(truth_bot, mod_bot)\n",
    "    R2_ensemble_bot[iensemble] = L2_R(truth_bot, mod_bot)\n",
    "\n",
    "print('full panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_full)\n",
    "max_val = np.max(corr_ensemble_full)\n",
    "min_val = np.min(corr_ensemble_full)\n",
    "std_val = np.std(corr_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('full panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_full)\n",
    "max_val = np.max(R2_ensemble_full)\n",
    "min_val = np.min(R2_ensemble_full)\n",
    "std_val = np.std(R2_ensemble_full)\n",
    "latex_table_row=f', full & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, correlation:')\n",
    "mean_val = np.mean(corr_ensemble_mid)\n",
    "max_val = np.max(corr_ensemble_mid)\n",
    "min_val = np.min(corr_ensemble_mid)\n",
    "std_val = np.std(corr_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)\n",
    "print('midjet panel, R2:')\n",
    "mean_val = np.mean(R2_ensemble_mid)\n",
    "max_val = np.max(R2_ensemble_mid)\n",
    "min_val = np.min(R2_ensemble_mid)\n",
    "std_val = np.std(R2_ensemble_mid)\n",
    "latex_table_row=f', mid & {mean_val:.4f} & {max_val:.4f} & {min_val:.4f} & {std_val:.4f} \\\\\\\\'\n",
    "print(latex_table_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML (based on the latest module pytorch)",
   "language": "python",
   "name": "ml-aim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b958b4221dbd89eccc41b3ab284a3e7443193eeb047d11fe6a91aac279bd724"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
