{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We try another shallower UNet with a receptive field comparable to the IT wavelength. \n",
    "#As the grid resolution is 4 km and the mode-1 tidal wavelength is about 230 km maximum.   \n",
    "#Does not save the checkpoints  -- saving seems to cause some overflow.\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from netCDF4 import Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score as R2\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score as r2\n",
    "from copy import deepcopy\n",
    "import utils\n",
    "from unet import UNet_nobatchnorm\n",
    "from ShallowUNet_nobatchnorm import OnelayerUNet\n",
    "from scipy.stats import pearsonr\n",
    "#JU's addtion to automate inputs and outputs\n",
    "import helper_functions as hf\n",
    "import os\n",
    "def save_fn(var_input_list, var_output_list):\n",
    "    var_input_join  = '_and_'.join(var_input_list)\n",
    "    var_output_join = '_and_'.join(var_output_list)\n",
    "    return '{}_to_{}'.format(var_input_join, var_output_join)\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print ('Running on ', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many parameters are there in the original UNet used elsewhere in this project\n",
    "N_inp = 1\n",
    "N_out = 2\n",
    "\n",
    "model = UNet_nobatchnorm(N_inp, N_out, bilinear = True, Nbase = 16).cuda()\n",
    "input = torch.randn(1,N_inp,256,720).to(device) \n",
    "output = model(input)\n",
    "print('number of paramters in the 4-layer UNet with Nbase=16:', utils.nparams(model)/1e6, ' million params')\n",
    "\n",
    "\n",
    "model = OnelayerUNet(N_inp, N_out, bilinear = True, Nbase = 110).cuda()\n",
    "output = model(input)\n",
    "print('number of paramters in the 1-layer UNet with an increased Nbase:', utils.nparams(model)/1e6, ' million params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxEpochs =  300#small number is taken for debugging\n",
    "nensemble = 1 #How many training sessions are run for each configuration \n",
    "Nbase = 110 #experimented in the previous block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi #GPU usage should be maxed out during training; tune batch_size according to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/work/uo0780/u241359/project_tide_synergy/data/'\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print ('Running on ', device)\n",
    "\n",
    "Ntrain = np.sum([nc.dimensions['time_counter'].size for nc in nctrains], axis = 0)\n",
    "Ntest = np.sum([nc.dimensions['time_counter'].size for nc in nctest], axis = 0)\n",
    "\n",
    "print('number of training records:', Ntrain)\n",
    "print('number of testing records:', Ntest)\n",
    "\n",
    "numTrainFiles = len(nctrains)\n",
    "numRecsFile = nctrains[0].dimensions['time_counter'].size #How many snapshots in time in each data set there is\n",
    "print (numRecsFile)\n",
    "\n",
    "\n",
    "def preload_data(nctrains, total_records):\n",
    "    #total_records = Ntrain#sum(nc.dimensions['time_counter'].size for nc in nctrains)\n",
    "    #dimensions of data of the nc file.\n",
    "    max_height = 722\n",
    "    max_width = 258\n",
    "    all_input_data = np.zeros((total_records, N_inp, max_height, max_width))*np.nan\n",
    "    all_output_data = np.zeros((total_records, N_out, max_height, max_width))*np.nan\n",
    "    current_index = 0\n",
    "    for ncindex, ncdata in enumerate(nctrains):\n",
    "        num_recs = ncdata.dimensions['time_counter'].size\n",
    "        rec_slice = slice(current_index, current_index + num_recs)\n",
    "        \n",
    "        for ind, var_name in enumerate(var_input_names):\n",
    "            data_slice = np.squeeze(ncdata.variables[var_name])\n",
    "            # print('data_slice shape:')\n",
    "            # print(data_slice.shape)        \n",
    "            #all_input_data[rec_slice, ind, :, :] = data_slice\n",
    "            #For some variables, the dimensions in (x, y) may be smaller than (max_height, max_width). Changing the code so that it adapts them.\n",
    "            # Get the actual dimensions of data_slice\n",
    "            slice_height, slice_width = data_slice.shape[-2], data_slice.shape[-1]\n",
    "            # Place data_slice into the corresponding slice of all_input_data\n",
    "            all_input_data[rec_slice, ind, :slice_height, :slice_width] = data_slice\n",
    "    \n",
    "\n",
    "        for ind, var_name in enumerate(var_output_names):\n",
    "            data_slice = np.squeeze(ncdata.variables[var_name])\n",
    "            #all_output_data[rec_slice, ind, :, :] = data_slice\n",
    "            # Get the actual dimensions of data_slice\n",
    "            slice_height, slice_width = data_slice.shape[-2], data_slice.shape[-1]\n",
    "            # Place data_slice into the corresponding slice of all_input_data\n",
    "            all_output_data[rec_slice, ind, :slice_height, :slice_width] = data_slice\n",
    "\n",
    "        current_index += num_recs\n",
    "        \n",
    "    return all_input_data, all_output_data\n",
    "    \n",
    "# Modify the loadtrain function to pull data from preloaded memory\n",
    "def loaddata_preloaded_train(index, batch_size, all_input_data, all_output_data):\n",
    "    rec_slice = slice(index, index + batch_size)\n",
    "    lim = 720\n",
    "    width = 256\n",
    "    yslice = slice(0, lim)\n",
    "    xslice = slice(0, width)\n",
    "    # print('rec_slice is:')\n",
    "    # print(rec_slice)\n",
    "    # print('mean of squared values of loaded input data:')\n",
    "    # print(\"{0:0.32f}\".format(np.nanmean(all_input_data[rec_slice, :, yslice, xslice]**2)))\n",
    "    return (all_input_data[rec_slice, :, yslice, xslice], \n",
    "            all_output_data[rec_slice, :, yslice, xslice])\n",
    "#Load test data as one single batch\n",
    "def loaddata_preloaded_test(all_input_data, all_output_data):\n",
    "    #rec_slice = slice(index, index + batch_size)\n",
    "    lim = 720\n",
    "    width = 256\n",
    "    yslice = slice(0, lim)\n",
    "    xslice = slice(0, width)\n",
    "    # print('rec_slice is:')\n",
    "    # print(rec_slice)\n",
    "    # print('mean of squared values of loaded input data:')\n",
    "    # print(\"{0:0.32f}\".format(np.nanmean(all_input_data[rec_slice, :, yslice, xslice]**2)))\n",
    "    return (all_input_data[:, :, yslice, xslice], \n",
    "            all_output_data[:, :, yslice, xslice])\n",
    "\n",
    "\n",
    "def load_variable(ncdata, ncindex, variable, rec_slice, yslice, xslice):\n",
    "    data_squeezed = np.squeeze(ncdata[ncindex].variables[variable])\n",
    "    return data_squeezed[rec_slice, yslice, xslice]\n",
    "\n",
    "\n",
    "# def loadtest():\n",
    "#     var_input = np.ones([150, N_inp, 720, 256])\n",
    "#     var_output = np.ones([150, N_out, 720, 256])\n",
    "\n",
    "#     for ind, var_name in enumerate(var_input_names):\n",
    "#         data_squeezed = np.squeeze(nctest.variables[var_name])\n",
    "#         var_input[:, ind, :, :] = data_squeezed[rectest_slice, ytest_slice, xtest_slice]\n",
    "#     for ind, var_name in enumerate(var_output_names):\n",
    "#         data_squeezed = np.squeeze(nctest.variables[var_name])\n",
    "#         var_output[:, ind, :, :] = data_squeezed[rectest_slice, ytest_slice, xtest_slice]\n",
    "#     return var_input, var_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(var_input_names, var_output_names, save_fn_prefix, N_inp, N_out, iensemble, R2_all, corr_all):\n",
    "    ytest_slice = slice(0, 720)\n",
    "    xtest_slice = slice(0, 256)\n",
    "    rectest_slice = slice(0, 150)\n",
    "\n",
    "    def totorch(x):\n",
    "        return torch.tensor(x, dtype = torch.float).cuda()\n",
    "\n",
    "    model = OnelayerUNet(N_inp, N_out, bilinear = True, Nbase = Nbase).cuda()\n",
    "    #model = torch.compile(UNet(N_inp, N_out, bilinear = True, Nbase = Nbase).cuda())\n",
    "\n",
    "    if iensemble == 0:\n",
    "        input = torch.randn(1,N_inp,256,720).to(device) \n",
    "        output = model(input)\n",
    "        print('Model has ', utils.nparams(model)/1e6, ' million params')\n",
    "\n",
    "    # for index in range(0, Ntrain, batch_size):\n",
    "    #     inp, out = loadtrain_preloaded(index, batch_size, all_train_input, all_train_output)\n",
    "    #     print(inp.shape, out.shape)\n",
    "#         print(np.nanmean(inp**2), np.max(inp**2), inp.shape)\n",
    "#         print(np.nanmean(out**2), np.max(out**2), inp.shape)\n",
    "\n",
    "    inp_test, out_test = loaddata_preloaded_test(all_test_input, all_test_output)\n",
    "    #inp, out_test = loadtest()\n",
    "    # print('shapes of input and output TEST data:')\n",
    "    # print(inp_test.shape, out_test.shape)\n",
    "    with torch.no_grad():\n",
    "        inp_test = totorch(inp_test)\n",
    "\n",
    "    Tcycle = 10\n",
    "    criterion_train  = nn.L1Loss()\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=lr0, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-5*100) #increase weight_decay ***\n",
    "\n",
    "    r2_test = np.zeros(maxEpochs)\n",
    "    epochmin = []\n",
    "    maxr2l = []\n",
    "\n",
    "    learn = np.zeros(maxEpochs)\n",
    "    minloss = 1000\n",
    "    maxR2 = -1000\n",
    "    minlosscount = 0\n",
    "    perm = False\n",
    "\n",
    "    model_best = deepcopy(model)  # Initialize before the loop for safety\n",
    "\n",
    "    #print('Starting training loop')\n",
    "    for epoch in tqdm(range(maxEpochs)):\n",
    "        lr = utils.cosineSGDR(optim, epoch, T0=Tcycle, eta_min=0, eta_max=lr0, scheme = 'constant')  #captioning this seems to make the printed corr larger??***\n",
    "        model.train()\n",
    "        index_perm = np.arange(0, Ntrain, batch_size)\n",
    "        \n",
    "        if perm:\n",
    "            index_perm = np.random.permutation(index_perm)\n",
    "        \n",
    "        for index in index_perm:\n",
    "            inp, out = loaddata_preloaded_train(index, batch_size, all_train_input, all_train_output)            \n",
    "#           inp, out = loadtrain(index, batch_size, nctrains)\n",
    "            inp, out = totorch(inp), totorch(out)\n",
    "            #continue #do this to pause the later operations to check how long it takes for the steps up to this \n",
    "            out_mod = model(inp)\n",
    "            loss = criterion_train(out.squeeze(), out_mod.squeeze())\n",
    "            #Set gradient to zero\n",
    "            optim.zero_grad()\n",
    "            #Compute gradients       \n",
    "            loss.backward()\n",
    "            #Update parameters with new gradient\n",
    "            optim.step()\n",
    "            #Record train loss\n",
    "            #scheduler.step()\n",
    "          \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            #model_cpu = model.to('cpu')\n",
    "            #out_mod = model_cpu(inp_test.to('cpu'))\n",
    "            out_mod=model(inp_test)\n",
    "            \n",
    "            r2 = R2(out_test.flatten(), (out_mod).cpu().numpy().flatten())\n",
    "            r2_test[epoch] = r2\n",
    "            #print('Debugging: R2 of current epoch:', r2)#Debugging\n",
    "            #record current best model and best predictions\n",
    "            if maxR2 <  r2:\n",
    "                maxR2 = r2\n",
    "                epochmin.append(epoch)\n",
    "                maxr2l.append(maxR2)                \n",
    "                model_best = deepcopy(model)\n",
    "                corr, pval = pearsonr(out_test.flatten(), (out_mod).cpu().numpy().flatten())\n",
    "                print('R2:', r2, ' corr: ', corr, ' pval: ', pval)\n",
    "            #model = model_cpu.to(device)\n",
    "\n",
    "    #_, out_test = loadtest()\n",
    "    model_best.eval()\n",
    "    with torch.no_grad():\n",
    "    #     inp_test = totorch(inp)\n",
    "        model_best.to('cpu') #added by HW \n",
    "        out_mod = model_best(inp_test.to('cpu')).detach().cpu().numpy()\n",
    "\n",
    "    R2_all[iensemble]=R2(out_test.flatten(), out_mod.flatten())\n",
    "    corr_all[iensemble]=pearsonr(out_test.flatten(), out_mod.flatten())[0]\n",
    "    # print('Best model R2:', R2_all[iensemble])#pearsonr(out_test.flatten(), out_mod.flatten())[0])\n",
    "    # print('Best model corr:', corr_all[iensemble])#R2(out_test.flatten(), out_mod.flatten()))\n",
    "\n",
    "    # Nx, Ny = out_test.shape[2:]; Nx, Ny\n",
    "\n",
    "    # print(out_mod.shape, 'outout model shape')\n",
    "    # dr = '/work/uo0780/u241359/project_tide_synergy/trainedmodels' #'./models/to_vel'\n",
    "    # os.makedirs(dr, exist_ok=True) # exist_ok=True allows the function to do nothing (i.e., not raise an error) if the directory already exists.\n",
    "    # fstr = f'{save_fn_prefix}_rp_{iensemble}'\n",
    "    # PATH = dr + f'/{fstr}.pth'\n",
    "    # torch.save(model_best.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean and variance of all input data:\n",
      "[0.03307104] [0.3119807]\n",
      "mean and variance of all output data:\n",
      "[-5.16228102e-04 -9.83592627e-05] [9.36516511e-05 1.01456128e-04]\n",
      "Model has  1.225182  million params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 11.33 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.90 GiB is free. Including non-PyTorch memory, this process has 77.34 GiB memory in use. Of the allocated memory 65.37 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m#Have checked that after these operations, the data is scaled to be zero mean and variance 1.\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m iensemble \u001b[38;5;129;01min\u001b[39;00m np.arange(nensemble):\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_input_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_output_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_fn_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_inp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miensemble\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR2_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorr_all\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mR2 from the best models in each run are:\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(R2_all)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mrun_model\u001b[39m\u001b[34m(var_input_names, var_output_names, save_fn_prefix, N_inp, N_out, iensemble, R2_all, corr_all)\u001b[39m\n\u001b[32m     71\u001b[39m model.eval()\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m#model_cpu = model.to('cpu')\u001b[39;00m\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m#out_mod = model_cpu(inp_test.to('cpu'))\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     out_mod=\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     r2 = R2(out_test.flatten(), (out_mod).cpu().numpy().flatten())\n\u001b[32m     78\u001b[39m     r2_test[epoch] = r2\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sw/spack-levante/miniforge3-24.11.3-2-Linux-x86_64-hbhytx/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sw/spack-levante/miniforge3-24.11.3-2-Linux-x86_64-hbhytx/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/uo0780/u241359/project_tide_synergy/codes/ShallowUNet_nobatchnorm.py:71\u001b[39m, in \u001b[36mOnelayerUNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     x1 = \u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_reentrant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     x2 = checkpoint(\u001b[38;5;28mself\u001b[39m.down1, x1, use_reentrant=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     73\u001b[39m     x = checkpoint(\u001b[38;5;28mself\u001b[39m.up1, x2, x1, use_reentrant=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sw/spack-levante/miniforge3-24.11.3-2-Linux-x86_64-hbhytx/lib/python3.12/site-packages/torch/_compile.py:32\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     30\u001b[39m     fn.__dynamo_disable = disable_fn\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sw/spack-levante/miniforge3-24.11.3-2-Linux-x86_64-hbhytx/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m prior = _maybe_set_eval_frame(callback)\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    634\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sw/spack-levante/miniforge3-24.11.3-2-Linux-x86_64-hbhytx/lib/python3.12/site-packages/torch/utils/checkpoint.py:496\u001b[39m, in \u001b[36mcheckpoint\u001b[39m\u001b[34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;66;03m# Runs pre-forward logic\u001b[39;00m\n\u001b[32m    495\u001b[39m \u001b[38;5;28mnext\u001b[39m(gen)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m ret = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m# Runs post-forward logic\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sw/spack-levante/miniforge3-24.11.3-2-Linux-x86_64-hbhytx/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sw/spack-levante/miniforge3-24.11.3-2-Linux-x86_64-hbhytx/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/uo0780/u241359/project_tide_synergy/codes/unet_parts.py:39\u001b[39m, in \u001b[36mDoubleConv.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdouble_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sw/spack-levante/miniforge3-24.11.3-2-Linux-x86_64-hbhytx/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sw/spack-levante/miniforge3-24.11.3-2-Linux-x86_64-hbhytx/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sw/spack-levante/miniforge3-24.11.3-2-Linux-x86_64-hbhytx/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sw/spack-levante/miniforge3-24.11.3-2-Linux-x86_64-hbhytx/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sw/spack-levante/miniforge3-24.11.3-2-Linux-x86_64-hbhytx/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sw/spack-levante/miniforge3-24.11.3-2-Linux-x86_64-hbhytx/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sw/spack-levante/miniforge3-24.11.3-2-Linux-x86_64-hbhytx/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 11.33 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.90 GiB is free. Including non-PyTorch memory, this process has 77.34 GiB memory in use. Of the allocated memory 65.37 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "vi1 = 'ssh_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}_{}{}_OnelayerUNet_'.format(vi1, vo1, vo2)\n",
    "var_input_names = [vi1]\n",
    "var_output_names = [vo1, vo2]\n",
    "\n",
    "batch_size = 1 #maximizing it so that the GPU memory maxes out. Needs to be divisible by Ntrain. Otherwise there will be size mismatch issues.\n",
    "\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "lr0 = 0.005*10/batch_size #Roughly should scale inversely to batch_size\n",
    "\n",
    "#Recording performance metrics on test data after eaching training cycle\n",
    "R2_all = np.zeros(nensemble)\n",
    "corr_all = np.zeros(nensemble)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "print(\"mean and variance of all input data:\")\n",
    "print(mean_input,var_input)\n",
    "print(\"mean and variance of all output data:\")\n",
    "print(mean_output,var_output)\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "#Have checked that after these operations, the data is scaled to be zero mean and variance 1.\n",
    "\n",
    "for iensemble in np.arange(nensemble):\n",
    "    run_model(var_input_names, var_output_names, save_fn_prefix, N_inp, N_out, iensemble, R2_all, corr_all)  \n",
    "print('R2 from the best models in each run are:')\n",
    "print(R2_all)\n",
    "print('corr from the best models in each run are:')\n",
    "print(corr_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vi1 = 'T_xy_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}_{}{}_OnelayerUNet_'.format(vi1, vo1, vo2)\n",
    "var_input_names = [vi1]\n",
    "var_output_names = [vo1, vo2]\n",
    "\n",
    "batch_size = 100 #maximizing it so that the GPU memory maxes out. Needs to be divisible by Ntrain. Otherwise there will be size mismatch issues.\n",
    "\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "lr0 = 0.005*10/batch_size #Roughly should scale inversely to batch_size\n",
    "\n",
    "#Recording performance metrics on test data after eaching training cycle\n",
    "R2_all = np.zeros(nensemble)\n",
    "corr_all = np.zeros(nensemble)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "print(\"mean and variance of all input data:\")\n",
    "print(mean_input,var_input)\n",
    "print(\"mean and variance of all output data:\")\n",
    "print(mean_output,var_output)\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "#Have checked that after these operations, the data is scaled to be zero mean and variance 1.\n",
    "\n",
    "for iensemble in np.arange(nensemble):\n",
    "    run_model(var_input_names, var_output_names, save_fn_prefix, N_inp, N_out, iensemble, R2_all, corr_all)  \n",
    "print('R2 from the best models in each run are:')\n",
    "print(R2_all)\n",
    "print('corr from the best models in each run are:')\n",
    "print(corr_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi1 = 'u_xy_ins'\n",
    "vi2 = 'v_xy_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}{}_{}{}_OnelayerUNet_'.format(vi1, vi2, vo1, vo2)\n",
    "var_input_names = [vi1, vi2]\n",
    "var_output_names = [vo1, vo2]\n",
    "\n",
    "batch_size = 60 #maximizing it so that the GPU memory maxes out. Needs to be divisible by Ntrain. Otherwise there will be size mismatch issues.\n",
    "\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "lr0 = 0.005*10/batch_size #Roughly should scale inversely to batch_size\n",
    "\n",
    "#Recording performance metrics on test data after eaching training cycle\n",
    "R2_all = np.zeros(nensemble)\n",
    "corr_all = np.zeros(nensemble)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "print(\"mean and variance of all input data:\")\n",
    "print(mean_input,var_input)\n",
    "print(\"mean and variance of all output data:\")\n",
    "print(mean_output,var_output)\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "#Have checked that after these operations, the data is scaled to be zero mean and variance 1.\n",
    "\n",
    "for iensemble in np.arange(nensemble):\n",
    "    run_model(var_input_names, var_output_names, save_fn_prefix, N_inp, N_out, iensemble, R2_all, corr_all)  \n",
    "print('R2 from the best models in each run are:')\n",
    "print(R2_all)\n",
    "print('corr from the best models in each run are:')\n",
    "print(corr_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi1 = 'ssh_ins'\n",
    "vi2 = 'T_xy_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}{}_{}{}_OnelayerUNet_'.format(vi1, vi2, vo1, vo2)\n",
    "var_input_names = [vi1, vi2]\n",
    "var_output_names = [vo1, vo2]\n",
    "\n",
    "batch_size = 60 #maximizing it so that the GPU memory maxes out. Needs to be divisible by Ntrain. Otherwise there will be size mismatch issues.\n",
    "\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "lr0 = 0.005*10/batch_size #Roughly should scale inversely to batch_size\n",
    "\n",
    "#Recording performance metrics on test data after eaching training cycle\n",
    "R2_all = np.zeros(nensemble)\n",
    "corr_all = np.zeros(nensemble)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "print(\"mean and variance of all input data:\")\n",
    "print(mean_input,var_input)\n",
    "print(\"mean and variance of all output data:\")\n",
    "print(mean_output,var_output)\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "#Have checked that after these operations, the data is scaled to be zero mean and variance 1.\n",
    "\n",
    "for iensemble in np.arange(nensemble):\n",
    "    run_model(var_input_names, var_output_names, save_fn_prefix, N_inp, N_out, iensemble, R2_all, corr_all)  \n",
    "print('R2 from the best models in each run are:')\n",
    "print(R2_all)\n",
    "print('corr from the best models in each run are:')\n",
    "print(corr_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi1 = 'ssh_ins'\n",
    "vi2 = 'u_xy_ins'\n",
    "vi3 = 'v_xy_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}{}{}_{}{}_nobatchnorm'.format(vi1, vi2, vi3, vo1, vo2)\n",
    "\n",
    "batch_size = 40 #maximizing it so that the GPU memory maxes out. Needs to be divisible by Ntrain. Otherwise there will be size mismatch issues.\n",
    "\n",
    "var_input_names = [vi1, vi2, vi3]\n",
    "var_output_names = [vo1, vo2]\n",
    "\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "lr0 = 0.005*10/batch_size #Roughly should scale inversely to batch_size\n",
    "def preload_data(nctrains, total_records):\n",
    "    #total_records = Ntrain#sum(nc.dimensions['time_counter'].size for nc in nctrains)\n",
    "    #dimensions of data of the nc file.\n",
    "    max_height = 722\n",
    "    max_width = 258\n",
    "    all_input_data = np.zeros((total_records, N_inp, max_height, max_width))\n",
    "    all_output_data = np.zeros((total_records, N_out, max_height, max_width))\n",
    "    current_index = 0\n",
    "    for ncindex, ncdata in enumerate(nctrains):\n",
    "        num_recs = ncdata.dimensions['time_counter'].size\n",
    "        rec_slice = slice(current_index, current_index + num_recs)\n",
    "        \n",
    "        for ind, var_name in enumerate(var_input_names):\n",
    "            data_slice = np.squeeze(ncdata.variables[var_name])\n",
    "            # print('data_slice shape:')\n",
    "            # print(data_slice.shape)        \n",
    "            #all_input_data[rec_slice, ind, :, :] = data_slice\n",
    "            #For some variables, the dimensions in (x, y) may be smaller than (max_height, max_width). Changing the code so that it adapts them.\n",
    "            # Get the actual dimensions of data_slice\n",
    "            slice_height, slice_width = data_slice.shape[-2], data_slice.shape[-1]\n",
    "            # Place data_slice into the corresponding slice of all_input_data\n",
    "            all_input_data[rec_slice, ind, :slice_height, :slice_width] = data_slice\n",
    "    \n",
    "\n",
    "        for ind, var_name in enumerate(var_output_names):\n",
    "            data_slice = np.squeeze(ncdata.variables[var_name])\n",
    "            #all_output_data[rec_slice, ind, :, :] = data_slice\n",
    "            # Get the actual dimensions of data_slice\n",
    "            slice_height, slice_width = data_slice.shape[-2], data_slice.shape[-1]\n",
    "            # Place data_slice into the corresponding slice of all_input_data\n",
    "            all_output_data[rec_slice, ind, :slice_height, :slice_width] = data_slice\n",
    "\n",
    "        current_index += num_recs\n",
    "        \n",
    "    return all_input_data, all_output_data\n",
    "#Recording performance metrics on test data after eaching training cycle\n",
    "R2_all = np.zeros(nensemble)\n",
    "corr_all = np.zeros(nensemble)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "print(\"mean and variance of all input data:\")\n",
    "print(mean_input,var_input)\n",
    "print(\"mean and variance of all output data:\")\n",
    "print(mean_output,var_output)\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "#Have checked that after these operations, the data is scaled to be zero mean and variance 1.\n",
    "\n",
    "for iensemble in np.arange(nensemble):\n",
    "    run_model(var_input_names, var_output_names, save_fn_prefix, N_inp, N_out, iensemble, R2_all, corr_all)  \n",
    "print('R2 from the best models in each run are:')\n",
    "print(R2_all)\n",
    "print('corr from the best models in each run are:')\n",
    "print(corr_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi1 = 'T_xy_ins'\n",
    "vi2 = 'u_xy_ins'\n",
    "vi3 = 'v_xy_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}{}{}_{}{}_OnelayerUNet_'.format(vi1, vi2, vi3, vo1, vo2)\n",
    "\n",
    "batch_size = 40 #maximizing it so that the GPU memory maxes out. Needs to be divisible by Ntrain. Otherwise there will be size mismatch issues.\n",
    "\n",
    "var_input_names = [vi1, vi2, vi3]\n",
    "var_output_names = [vo1, vo2]\n",
    "\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "lr0 = 0.005*10/batch_size #Roughly should scale inversely to batch_size\n",
    "def preload_data(nctrains, total_records):\n",
    "    #total_records = Ntrain#sum(nc.dimensions['time_counter'].size for nc in nctrains)\n",
    "    #dimensions of data of the nc file.\n",
    "    max_height = 722\n",
    "    max_width = 258\n",
    "    all_input_data = np.zeros((total_records, N_inp, max_height, max_width))\n",
    "    all_output_data = np.zeros((total_records, N_out, max_height, max_width))\n",
    "    current_index = 0\n",
    "    for ncindex, ncdata in enumerate(nctrains):\n",
    "        num_recs = ncdata.dimensions['time_counter'].size\n",
    "        rec_slice = slice(current_index, current_index + num_recs)\n",
    "        \n",
    "        for ind, var_name in enumerate(var_input_names):\n",
    "            data_slice = np.squeeze(ncdata.variables[var_name])\n",
    "            # print('data_slice shape:')\n",
    "            # print(data_slice.shape)        \n",
    "            #all_input_data[rec_slice, ind, :, :] = data_slice\n",
    "            #For some variables, the dimensions in (x, y) may be smaller than (max_height, max_width). Changing the code so that it adapts them.\n",
    "            # Get the actual dimensions of data_slice\n",
    "            slice_height, slice_width = data_slice.shape[-2], data_slice.shape[-1]\n",
    "            # Place data_slice into the corresponding slice of all_input_data\n",
    "            all_input_data[rec_slice, ind, :slice_height, :slice_width] = data_slice\n",
    "    \n",
    "\n",
    "        for ind, var_name in enumerate(var_output_names):\n",
    "            data_slice = np.squeeze(ncdata.variables[var_name])\n",
    "            #all_output_data[rec_slice, ind, :, :] = data_slice\n",
    "            # Get the actual dimensions of data_slice\n",
    "            slice_height, slice_width = data_slice.shape[-2], data_slice.shape[-1]\n",
    "            # Place data_slice into the corresponding slice of all_input_data\n",
    "            all_output_data[rec_slice, ind, :slice_height, :slice_width] = data_slice\n",
    "\n",
    "        current_index += num_recs\n",
    "        \n",
    "    return all_input_data, all_output_data\n",
    "#Recording performance metrics on test data after eaching training cycle\n",
    "R2_all = np.zeros(nensemble)\n",
    "corr_all = np.zeros(nensemble)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "print(\"mean and variance of all input data:\")\n",
    "print(mean_input,var_input)\n",
    "print(\"mean and variance of all output data:\")\n",
    "print(mean_output,var_output)\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "#Have checked that after these operations, the data is scaled to be zero mean and variance 1.\n",
    "\n",
    "for iensemble in np.arange(nensemble):\n",
    "    run_model(var_input_names, var_output_names, save_fn_prefix, N_inp, N_out, iensemble, R2_all, corr_all)  \n",
    "print('R2 from the best models in each run are:')\n",
    "print(R2_all)\n",
    "print('corr from the best models in each run are:')\n",
    "print(corr_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi1 = 'ssh_ins'\n",
    "vi2 = 'T_xy_ins'\n",
    "vi3 = 'u_xy_ins'\n",
    "vi4 = 'v_xy_ins'\n",
    "\n",
    "vo1 = 'ssh_cos'\n",
    "vo2 = 'ssh_sin'\n",
    "\n",
    "save_fn_prefix  = 'any_{}{}{}{}_{}{}_OnelayerUNet_'.format(vi1, vi2, vi3, vi4, vo1, vo2)\n",
    "var_input_names = [vi1, vi2, vi3, vi4]\n",
    "var_output_names = [vo1, vo2]\n",
    "\n",
    "batch_size = 40 #maximizing it so that the GPU memory maxes out. Needs to be divisible by Ntrain. Otherwise there will be size mismatch issues.\n",
    "\n",
    "N_inp = len(var_input_names)\n",
    "N_out = len(var_output_names)\n",
    "\n",
    "lr0 = 0.005*10/batch_size #Roughly should scale inversely to batch_size\n",
    "\n",
    "#Recording performance metrics on test data after eaching training cycle\n",
    "R2_all = np.zeros(nensemble)\n",
    "corr_all = np.zeros(nensemble)\n",
    "\n",
    "nctrains, nctest = hf.load_data_from_nc_as_lists(root_dir)\n",
    "\n",
    "all_train_input, all_train_output = preload_data(nctrains, Ntrain)\n",
    "all_test_input, all_test_output = preload_data(nctest, Ntest)\n",
    "\n",
    "#Normalize data\n",
    "#Compute mean and variance for normalization\n",
    "mean_input=np.nanmean(np.concatenate((all_train_input, all_test_input), axis=0),axis=(0, 2, 3))\n",
    "mean_output=np.nanmean(np.concatenate((all_train_output, all_test_output), axis=0),axis=(0, 2, 3))\n",
    "#Subtract the data with their means\n",
    "all_train_input=all_train_input-mean_input[None, :, None, None]\n",
    "all_train_output=all_train_output-mean_output[None, :, None, None]\n",
    "all_test_input=all_test_input-mean_input[None, :, None, None]\n",
    "all_test_output=all_test_output-mean_output[None, :, None, None]\n",
    "#Compute the variances\n",
    "var_input=np.nanmean((np.concatenate((all_train_input, all_test_input), axis=0))**2,axis=(0, 2, 3))\n",
    "var_output=np.nanmean((np.concatenate((all_train_output, all_test_output), axis=0))**2,axis=(0, 2, 3))\n",
    "print(\"mean and variance of all input data:\")\n",
    "print(mean_input,var_input)\n",
    "print(\"mean and variance of all output data:\")\n",
    "print(mean_output,var_output)\n",
    "#Scale the data so that they have variance of 1\n",
    "all_train_input=all_train_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_train_output=all_train_output/np.sqrt(var_output[None, :, None, None])\n",
    "all_test_input=all_test_input/np.sqrt(var_input[None, :, None, None])\n",
    "all_test_output=all_test_output/np.sqrt(var_output[None, :, None, None])\n",
    "#Have checked that after these operations, the data is scaled to be zero mean and variance 1.\n",
    "\n",
    "for iensemble in np.arange(nensemble):\n",
    "    run_model(var_input_names, var_output_names, save_fn_prefix, N_inp, N_out, iensemble, R2_all, corr_all)  \n",
    "print('R2 from the best models in each run are:')\n",
    "print(R2_all)\n",
    "print('corr from the best models in each run are:')\n",
    "print(corr_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML (based on the latest module pytorch)",
   "language": "python",
   "name": "ml-aim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b958b4221dbd89eccc41b3ab284a3e7443193eeb047d11fe6a91aac279bd724"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
